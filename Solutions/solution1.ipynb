{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findroutes(filename, limit_data=0, driver_id=0, prints=False):    \n",
    "    import json\n",
    "    import math\n",
    "    import numpy as np\n",
    "    import warnings\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    def load_json(filename):\n",
    "        with open(filename, 'r') as file:\n",
    "            data4 = json.load(file)\n",
    "        return data4\n",
    "\n",
    "    def load_data(actual_routes_file1, limit_actual_routes1=0, driver_id=0):\n",
    "        #load data and limit it as required\n",
    "        \n",
    "        #load driverdata\n",
    "        driver_data1=load_json(actual_routes_file1)\n",
    "\n",
    "        #limit data to only the driver that is selected\n",
    "        if driver_id!=0:\n",
    "            driver_data=[]\n",
    "            for route in driver_data1:\n",
    "                if route[\"driver\"]==driver_id:\n",
    "                    driver_data.append(route)\n",
    "            driver_data1=driver_data\n",
    "\n",
    "        #if a limit on total routes is set apply it\n",
    "        if limit_actual_routes1!=0:\n",
    "            driver_data1=driver_data1[:limit_actual_routes1]\n",
    "        return driver_data1\n",
    "\n",
    "    def find_numb_dim_and_conn(data):\n",
    "        #find the amount of dimensions (possible merch) for all the possible \n",
    "        #connections between cities \n",
    "        dim_count={}\n",
    "        standardroutes=set()\n",
    "        for route_info in data:\n",
    "            standardroutes.add(route_info[\"sroute\"])\n",
    "            for trip_info in route_info[\"route\"]:\n",
    "                \n",
    "                set_of_items=set()\n",
    "                \n",
    "                for merchandise in trip_info[\"merchandise\"]:\n",
    "                    set_of_items.add(merchandise)\n",
    "\n",
    "                conn_name=trip_info[\"from\"]+\"-\"+trip_info[\"to\"]\n",
    "                \n",
    "                if conn_name not in dim_count:\n",
    "                    dim_count[conn_name]=set_of_items\n",
    "                else:\n",
    "                    dim_count[conn_name].update(set_of_items)\n",
    "\n",
    "        #create a mapping method by converting the sets (first for speed) into\n",
    "        #tuples which take less memory than lists\n",
    "        #we do this so we can say that 'pens' is for example the first dimension\n",
    "        #and 'milk' the second\n",
    "        for conn_name in dim_count:\n",
    "            dim_count[conn_name]=(tuple(dim_count[conn_name]))\n",
    "        \n",
    "        return dim_count, len(standardroutes)\n",
    "\n",
    "    def createpoints(data, dim_count):   \n",
    "        #convert the data into lists of data points so the clustering can be applied\n",
    "        data_points={}\n",
    "        for conn_name in dim_count:\n",
    "            data_points[conn_name]=[]\n",
    "\n",
    "        for route_info in data:\n",
    "            for trip_info in route_info[\"route\"]:\n",
    "                conn_name=trip_info[\"from\"]+\"-\"+trip_info[\"to\"]\n",
    "                \n",
    "                temp_point=[0] * len(dim_count[conn_name])\n",
    "                \n",
    "                for merch in trip_info[\"merchandise\"]:\n",
    "                    index=dim_count[conn_name].index(merch)\n",
    "                    temp_point[index]=trip_info[\"merchandise\"][merch]\n",
    "                            \n",
    "                data_points[conn_name].append(temp_point)\n",
    "        \n",
    "        return data_points\n",
    "\n",
    "    def clustering(data_points):\n",
    "        #find all clusters\n",
    "\n",
    "        #ignore warnings temporarily for better readability\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "        def sample(upper_limit1, amount_of_samples1, data1):\n",
    "            rand_numbs=random.sample(range(0, upper_limit1), amount_of_samples1)\n",
    "            sample_space=[list(data1[i]) for i in rand_numbs]\n",
    "            return sample_space\n",
    "\n",
    "        def round_list(lst, decimal_places=0):\n",
    "            rounded_list = [round(element, decimal_places) for element in lst]\n",
    "            return rounded_list\n",
    "\n",
    "        ext_data_points={}\n",
    "        clusterinfo={}\n",
    "        count=0\n",
    "        for city in data_points:\n",
    "            count=count+1\n",
    "            max_expected_clusters=0\n",
    "            labeling={}\n",
    "            data = data_points[city]\n",
    "            #limit the sample space\n",
    "            datasize=len(data)\n",
    "            upper_limit=datasize\n",
    "            if datasize<1000:\n",
    "                amount_of_samples=min(datasize, 100)\n",
    "                max_expected_clusters=round(float(amount_of_samples)**(1/2))\n",
    "            elif datasize<10000:\n",
    "                amount_of_samples=round(datasize/10)\n",
    "                max_expected_clusters=round(float(amount_of_samples)**(1/2))\n",
    "\n",
    "            else:\n",
    "                amount_of_samples=round(datasize/100)\n",
    "                max_expected_clusters=round(float(amount_of_samples)**(1/2))\n",
    "\n",
    "            sample_space=[]\n",
    "            \n",
    "            #check if all the samplepoint    print(amount_of_samples)\n",
    "            #are the same and if they are try again 5 times\n",
    "            #if still the same pass the knowledge on\n",
    "            for _ in range(5):\n",
    "                one_point_marker=True\n",
    "                sample_space=sample(upper_limit, amount_of_samples, data)\n",
    "                if all(x == data[0] for x in sample_space) != True:\n",
    "                    one_point_marker=False\n",
    "                    break\n",
    "                    \n",
    "            # Calculate silhouette scores for different values of k using samples \n",
    "            silhouette_scores = []\n",
    "            #limit expected amount of clusters proportional to the amount of datapoints we can maybe say that for every 2% of dataset there may be a cluster existing\n",
    "            \n",
    "            K_range = range(2, max_expected_clusters)\n",
    "\n",
    "\n",
    "            \n",
    "            if one_point_marker==False and max_expected_clusters>2:\n",
    "                #loop through possible amount of clusters and stop for first silhouette with score >0.7\n",
    "                for k in K_range:\n",
    "                    try:\n",
    "                        scaler = MinMaxScaler()\n",
    "                        sample_space = scaler.fit_transform(sample_space)\n",
    "                        kmeans = KMeans(n_clusters=k)\n",
    "                        kmeans.fit(sample_space)\n",
    "                        labels = kmeans.labels_\n",
    "                        silhouette_scores.append(silhouette_score(sample_space, labels))\n",
    "                    except ConvergenceWarning as e:\n",
    "                        break\n",
    "                    if max(silhouette_scores)>0.7:\n",
    "                        break\n",
    "                \n",
    "                #determine k using the sampled space\n",
    "                max_ss=max(silhouette_scores)\n",
    "                if max_ss>0.6:\n",
    "                    k=silhouette_scores.index(max_ss)+2\n",
    "                else:\n",
    "                    k=1\n",
    "                    \n",
    "            else:\n",
    "                k=1\n",
    "            #now use kmeans to find the labels of the points for the entire set\n",
    "            scaler = MinMaxScaler()\n",
    "            data = scaler.fit_transform(data)\n",
    "            kmeans = KMeans(n_clusters=k)\n",
    "            kmeans.fit(data)\n",
    "\n",
    "            #build the extended data points set which also contain the labels of the points\n",
    "            labels=kmeans.labels_\n",
    "            \n",
    "            for index, point in enumerate(data_points[city]):\n",
    "                labeling[tuple(point)]=labels[index]\n",
    "            ext_data_points[city]=labeling\n",
    "            \n",
    "            #build a dataset which contains info about each cluster\n",
    "            for i in range(k):\n",
    "                temp={}\n",
    "                oneclusterdata=data[labels==i]\n",
    "                temp[\"count\"]=(len(oneclusterdata))\n",
    "                kmeans1=KMeans(n_clusters=1)\n",
    "                kmeans1.fit(oneclusterdata)\n",
    "                temp[\"inertia\"]=(kmeans1.inertia_)\n",
    "                temp[\"centroid\"]=tuple(round_list(scaler.inverse_transform(kmeans.cluster_centers_)[i]))\n",
    "                clusterinfo[city+\"-\"+str(i)]=temp\n",
    "        \n",
    "        return clusterinfo, ext_data_points\n",
    "\n",
    "    def build_dataset(data, dim_count, labeled_data_points):\n",
    "        #Now build the new, transformed dataset out of the old dataset and the gathered data\n",
    "        #the data we gathered are the mapping tool (dim_count) and the ext_data_points\n",
    "        dataset=[]\n",
    "        dim_map=dim_count\n",
    "        clust_map=labeled_data_points\n",
    "\n",
    "        for route_info in data:\n",
    "            conv_route=[]\n",
    "            cont_trips=[]\n",
    "            \n",
    "            for trip_info in route_info[\"route\"]:\n",
    "                conn_name=trip_info[\"from\"]+\"-\"+trip_info[\"to\"]\n",
    "                \n",
    "                temp_point=[0] * len(dim_map[conn_name])\n",
    "                \n",
    "                for merch in trip_info[\"merchandise\"]:\n",
    "                    index=dim_map[conn_name].index(merch)\n",
    "                    temp_point[index]=trip_info[\"merchandise\"][merch]\n",
    "                cluster=clust_map[conn_name][tuple(temp_point)]\n",
    "                trip_name=conn_name+\"-\"+str(cluster)\n",
    "                cont_trips.append(trip_name)\n",
    "                \n",
    "            conv_route.append(route_info[\"id\"])\n",
    "            conv_route.append(route_info[\"driver\"])\n",
    "            conv_route.append(route_info[\"sroute\"])\n",
    "            conv_route.append(tuple(cont_trips))\n",
    "            dataset.append(tuple(conv_route))\n",
    "            \n",
    "        return tuple(dataset)\n",
    "\n",
    "    def find_freq_routes(dataset,dim_map ,NS):\n",
    "\n",
    "        def FREQUENT_ITEMS(dataset,thresehold):\n",
    "            def subsequence(m,S):\n",
    "                #this function tells us wether a given sequence is a subsequence of a longer one.\n",
    "                #basically we are certifying if a given subroute is part of a bigger route in the dataset.\n",
    "                l=len(m)\n",
    "                L=len(S)\n",
    "                for i in range(0,L-l+1):\n",
    "                    if S[i:i+l]==m:\n",
    "                        return True\n",
    "                return False\n",
    "            \n",
    "            def match(x,y):\n",
    "                #A function that, given two trips, outputs wether they could be chained or not,\n",
    "                #for example, if the first trip ends in Milano, it tells you wether the second one\n",
    "                #starts in Milano too. In this way, we only create \"possible couples\" with the next function.\n",
    "                w1=x\n",
    "                l1=len(w1)\n",
    "                w1=w1[:l1-2]\n",
    "                index1=w1.index('-')\n",
    "                w1=w1[index1+1:]\n",
    "                \n",
    "                w2=y\n",
    "                l2=len(w2)\n",
    "                w2=w2[:l2-2]\n",
    "                index2=w2.index('-')\n",
    "                w2=w2[:index2]\n",
    "                \n",
    "                return w1==w2 \n",
    "        \n",
    "            def prune(X,thresehold):\n",
    "                #remove the singletons not frequent enough, depending on a certain thresehold. \n",
    "                #In this case, I've chosen 100. \n",
    "                #The function outputs a tuple with the frequent items and not frequently enough items.\n",
    "                \n",
    "                reduce=[]\n",
    "                for i in X:\n",
    "                    if X[i]<thresehold:\n",
    "                        reduce.append(((i,),X[i]))\n",
    "                for i in reduce:\n",
    "                    X.pop(i[0][0])\n",
    "                return (X,reduce)\n",
    "            \n",
    "            def couples(X):\n",
    "                #using the above function we create couples.It also gives us a list \n",
    "                #of the singletons that we could not extend, and a list of the ones we could.\n",
    "                #We need the latter in case the couples don't pass the prune in the next step;\n",
    "                #in that case, we would go back one step and offer the routes that created that couple.\n",
    "\n",
    "                nextcandidates=[]\n",
    "                extend=[]\n",
    "                for i in X:\n",
    "                    extend.append(((i,),X[i]))\n",
    "                lista=[]\n",
    "                for i in X:\n",
    "                    for j in X:\n",
    "                        if match(i,j):\n",
    "                            if ((i,),X[i]) in extend:\n",
    "                                extend.remove(((i,),X[i]))\n",
    "                            if ((j,),X[j]) in extend:\n",
    "                                extend.remove(((j,),X[j]))\n",
    "                                \n",
    "                            if (i,j) not in lista:\n",
    "                                lista.append((i,j))\n",
    "                            if ((i,),X[i]) not in nextcandidates:\n",
    "                                    nextcandidates.append(((i,),X[i]))\n",
    "                            if ((j,),X[j]) not in nextcandidates:\n",
    "                                    nextcandidates.append(((j,),X[j]))\n",
    "                return (lista,extend,nextcandidates)\n",
    "        \n",
    "            def combine(X):\n",
    "                #this function takes a list of tuples of the same length and combines them\n",
    "                #to create new tuples of length + 1. Furthermore, it only creates \"logical\"\n",
    "                #tuples. That is, it won't combine (1,2,3) with (1,2,5) but it will combine \n",
    "                #(1,2,3) with (2,3,4) to create (1,2,3,4). It also gives as a list of tuples\n",
    "                #we could not extend, and the ones we could, for analogous reasons as in the \n",
    "                #case of the function couples.\n",
    "                \n",
    "                extend=[]\n",
    "                for i in X:\n",
    "                    extend.append((i,X[i]))\n",
    "                nextcandidates=[]\n",
    "                lista=[]\n",
    "                if len(X)!=0:\n",
    "                    for i in X:\n",
    "                        l=len(i)\n",
    "                        break\n",
    "                    for tuple1 in X:\n",
    "                        for tuple2 in X:\n",
    "                            if tuple2[:l-1] == tuple1[1:]:\n",
    "                                newtuple=tuple1 + (tuple2[l-1],)\n",
    "                                if (tuple1,X[tuple1]) in extend:\n",
    "                                    extend.remove((tuple1,X[tuple1]))\n",
    "                                if (tuple2,X[tuple2]) in extend:\n",
    "                                    extend.remove((tuple2,X[tuple2]))\n",
    "                                if newtuple not in lista:\n",
    "                                    lista.append(newtuple)\n",
    "                                if (tuple1,X[tuple1]) not in nextcandidates:\n",
    "                                    nextcandidates.append((tuple1,X[tuple1]))\n",
    "                                if (tuple2,X[tuple2]) not in nextcandidates:\n",
    "                                    nextcandidates.append((tuple2,X[tuple2]))\n",
    "                return (lista,extend,nextcandidates)\n",
    "        \n",
    "            def find(candidates,X):\n",
    "                #given possible frequent tuples it creates a dictionary with the occurrences\n",
    "                #of such tuples as subsequences in the dataset.\n",
    "\n",
    "                freq={}\n",
    "                for i in candidates:\n",
    "                    for j in X:\n",
    "                        if subsequence(i,j[3]):\n",
    "                            if i in freq:\n",
    "                                freq[i]=freq[i]+1\n",
    "                            else:\n",
    "                                freq[i]=1\n",
    "                return freq\n",
    "            \n",
    "            def offeroutes(OFFER):\n",
    "                #It takes all of the cluster-routes and convert them into actual routes.\n",
    "\n",
    "                def cluster_to_info(x,clusterinfo,dim_map):\n",
    "                    #It takes a trip-clusters and converts it into an actual trip.\n",
    "                    \n",
    "                    w1=x\n",
    "                    index1=w1.index('-')\n",
    "                    w1=w1[index1+1:]\n",
    "                    index3=w1.index('-')\n",
    "                    w1=w1[:index3]\n",
    "                    \n",
    "                    w2=x\n",
    "                    index2=w2.index('-')\n",
    "                    w2=w2[:index2]\n",
    "                    \n",
    "                    trip={}\n",
    "                    \n",
    "                    trip['from']=w2\n",
    "                    trip['to']=w1\n",
    "                    \n",
    "                    info=clusterinfo[x]['centroid']\n",
    "            \n",
    "                    merch=dim_map[w2+\"-\"+w1]\n",
    "                    s={}\n",
    "                    for i in range(len(merch)):\n",
    "                        if info[i]!=0:\n",
    "                            s[merch[i]] = info[i]\n",
    "                            \n",
    "                    trip['merchandise']=s\n",
    "\n",
    "                    \n",
    "                    return trip\n",
    "\n",
    "                lista=[]\n",
    "                H=len(OFFER)\n",
    "                for i in range(H):\n",
    "                    ruta={}\n",
    "                    ruta['id']='s'+str(i+1)\n",
    "                    TRIPS=[]\n",
    "                    for j in OFFER[i][0]:\n",
    "                        trip= cluster_to_info(j,clusterinfo,dim_map)\n",
    "                        TRIPS.append(trip)\n",
    "                    ruta['route']=TRIPS\n",
    "                    lista.append((ruta,OFFER[i][1]))\n",
    "                L=[]\n",
    "                for i in lista:\n",
    "                    L.append(i[1])\n",
    "                L.sort()\n",
    "                L=L[::-1]\n",
    "                Lista=[]\n",
    "                for i in L:\n",
    "                    for j in lista:\n",
    "                        if i==j[1]:\n",
    "                            Lista.append(j[0])\n",
    "                            lista.remove(j)\n",
    "                            break   \n",
    "                for i in range(H):\n",
    "                    Lista[i]['id']='s'+str(i+1)\n",
    "                return Lista\n",
    "            \n",
    "            OFFER=[]    \n",
    "            freq={}\n",
    "            for x in dataset:\n",
    "                for i in x[3]:\n",
    "                    if i not in freq:\n",
    "                        freq[i] = 1\n",
    "                    else:\n",
    "                        freq[i] = freq[i] +1\n",
    "\n",
    "            #we can also use clusterinfo to get freq\n",
    "            #freq={}\n",
    "            #for i in clusterinfo:\n",
    "            #    freq[i]=clusterinfo[i]['count']\n",
    "            \n",
    "            S = prune(freq,thresehold)[0]\n",
    "            (candidates,extend,nextcandidates)=couples(S)\n",
    "            OFFER=OFFER + extend\n",
    "            \n",
    "            new={}\n",
    "            while len(candidates)!=0:\n",
    "                new=find(candidates,dataset)\n",
    "                (S,H) = prune(new,thresehold)\n",
    "                count=0\n",
    "                for j in nextcandidates:\n",
    "                    for i in H:\n",
    "                        if subsequence(j[0],i[0]):\n",
    "                            OFFER=OFFER + [j]\n",
    "                            count=count+1\n",
    "                    if count==0:\n",
    "                        for i in S:\n",
    "                            if subsequence(j[0],i):\n",
    "                                count=count+1\n",
    "                    if count==0:\n",
    "                        OFFER=OFFER + [j]\n",
    "                        \n",
    "                (candidates,extend,nextcandidates)=combine(S)\n",
    "                OFFER = OFFER+ extend\n",
    "            return offeroutes(OFFER)\n",
    "            \n",
    "        #assumes that there are at least possible routes as there are standard routes\n",
    "        n=math.floor(math.log2(NS))\n",
    "        \n",
    "        OFFER= FREQUENT_ITEMS(dataset,len(dataset)/(2**n))\n",
    "        while len(OFFER)<NS*2:\n",
    "            n=n+1\n",
    "            OFFER=FREQUENT_ITEMS(dataset,len(dataset)/(2**n))\n",
    "        return (OFFER[:NS])\n",
    "\n",
    "\n",
    "    if (prints==True): print(\"Loading data\")\n",
    "    data = load_data(filename, limit_data, driver_id)\n",
    "\n",
    "    if (prints==True): print(\"Finding dimension sizes\")\n",
    "    [dim_count, number_sroutes]=find_numb_dim_and_conn(data)\n",
    "\n",
    "    if (prints==True): print(\"Creating points\")\n",
    "    data_points=createpoints(data, dim_count)\n",
    "\n",
    "    if (prints==True): print(\"Clustering points\")\n",
    "    [clusterinfo, labels]=clustering(data_points)\n",
    "\n",
    "    if (prints==True): print(\"Rebuild dataset\")\n",
    "    dataset=build_dataset(data, dim_count, labels)\n",
    "\n",
    "    if (prints==True): print(\"Finding frequent routes\")\n",
    "    result=find_freq_routes(dataset, dim_count, number_sroutes)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Finding dimension sizes\n",
      "Creating points\n",
      "Clustering points\n",
      "Rebuild dataset\n",
      "Finding frequent routes\n",
      "[{'id': 's1', 'route': [{'from': 'Milano', 'to': 'Genova', 'merchandise': {'Meat': 22.0, 'Pasta': 25.0, 'Tea': 16.0}}, {'from': 'Genova', 'to': 'Siena', 'merchandise': {'Meat': 12.0, 'Butter': 3.0, 'Bread': 8.0, 'Pens': 17.0, 'Pasta': 27.0, 'Honey': 11.0}}, {'from': 'Siena', 'to': 'Trento', 'merchandise': {'Apples': 20.0, 'Rice': 3.0, 'Carrots': 6.0, 'Tomatoes': 6.0, 'Honey': 25.0}}]}]\n"
     ]
    }
   ],
   "source": [
    "filename=\"/home/felix/Documents/Python/DataMining/DataMining/data/actual.json\"\n",
    "result=findroutes(filename, 100, \"C\" , prints=True)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
