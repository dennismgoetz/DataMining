{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Get the current working directory, select file path for actual routes in subfolder 'data'\n",
    "current_directory = os.getcwd()\n",
    "subfolder_path = 'data'\n",
    "file_name = \"actual.json\"\n",
    "file_path = os.path.join(current_directory, subfolder_path, file_name)\n",
    "prints = True\n",
    "\n",
    "#import data\n",
    "if(prints==True): print(\"loading data into memory\")\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "if(prints==True): print(\"loaded \" + str(len(data)) + \" routes into memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the amount of dimensions (possible merch) for all the possible \n",
    "#connections between cities \n",
    "if(prints==True): print(\"determine subspaces and its dimensions\")\n",
    "dim_count = {}\n",
    "\n",
    "for route_info in data:\n",
    "    for trip_info in route_info[\"route\"]:\n",
    "        \n",
    "        set_of_items = set()\n",
    "        \n",
    "        for merchandise in trip_info[\"merchandise\"]:\n",
    "            set_of_items.add(merchandise)\n",
    "\n",
    "        conn_name = trip_info[\"from\"] + \"-\" + trip_info[\"to\"]\n",
    "        \n",
    "        if conn_name not in dim_count:\n",
    "            dim_count[conn_name] = set_of_items\n",
    "        else:\n",
    "            dim_count[conn_name].update(set_of_items)\n",
    "\n",
    "#create a mapping method by converting the sets (first for speed) into\n",
    "#tuples which take less memory than lists\n",
    "#we do this so we can say that 'pens' is for example the first dimension\n",
    "#and 'milk' the second  \n",
    "for conn_name in dim_count:\n",
    "    dim_count[conn_name] = (tuple(dim_count[conn_name]))\n",
    "if(prints==True): print(\"found \" + str(len(dim_count)) + \" subspaces:\")\n",
    "#if(prints==True): print(dim_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the data into lists of data points so the clustering can be applied\n",
    "if(prints==True): print(\"converting the data into data points for each subspaces\")\n",
    "data_points = {}\n",
    "for conn_name in dim_count:\n",
    "    data_points[conn_name] = []\n",
    "\n",
    "for route_info in data:\n",
    "    for trip_info in route_info[\"route\"]:\n",
    "        conn_name = trip_info[\"from\"] + \"-\" + trip_info[\"to\"]\n",
    "        \n",
    "        temp_point = [0] * len(dim_count[conn_name])\n",
    "        \n",
    "        for merch in trip_info[\"merchandise\"]:\n",
    "            index = dim_count[conn_name].index(merch)\n",
    "            temp_point[index] = trip_info[\"merchandise\"][merch]\n",
    "                    \n",
    "        data_points[conn_name].append(temp_point) #change to list again if needed\n",
    "if(prints==True): print(\"done\")\n",
    "#if(prints==True): print(data_points)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages/joblib/externals/loky/backend/context.py\", line 217, in _count_physical_cores\n",
      "    raise ValueError(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ConvergenceWarning' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 72\u001b[0m\n\u001b[1;32m     71\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39mk)\n\u001b[0;32m---> 72\u001b[0m kmeans\u001b[38;5;241m.\u001b[39mfit(sample_space)\n\u001b[1;32m     73\u001b[0m labels \u001b[38;5;241m=\u001b[39m kmeans\u001b[38;5;241m.\u001b[39mlabels_\n",
      "File \u001b[0;32m/opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1526\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;66;03m# run a k-means once\u001b[39;00m\n\u001b[0;32m-> 1526\u001b[0m labels, inertia, centers, n_iter_ \u001b[38;5;241m=\u001b[39m kmeans_single(\n\u001b[1;32m   1527\u001b[0m     X,\n\u001b[1;32m   1528\u001b[0m     sample_weight,\n\u001b[1;32m   1529\u001b[0m     centers_init,\n\u001b[1;32m   1530\u001b[0m     max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter,\n\u001b[1;32m   1531\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m   1532\u001b[0m     tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tol,\n\u001b[1;32m   1533\u001b[0m     n_threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_threads,\n\u001b[1;32m   1534\u001b[0m )\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# determine if these results are the best so far\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# we chose a new run if it has a better inertia and the clustering is\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;66;03m# different from the best so far (it's possible that the inertia is\u001b[39;00m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;66;03m# slightly better even if the clustering is the same with potentially\u001b[39;00m\n\u001b[1;32m   1540\u001b[0m \u001b[38;5;66;03m# permuted labels, due to rounding errors)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:740\u001b[0m, in \u001b[0;36m_kmeans_single_lloyd\u001b[0;34m(X, sample_weight, centers_init, max_iter, verbose, tol, n_threads)\u001b[0m\n\u001b[1;32m    728\u001b[0m         lloyd_iter(\n\u001b[1;32m    729\u001b[0m             X,\n\u001b[1;32m    730\u001b[0m             sample_weight,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    737\u001b[0m             update_centers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    738\u001b[0m         )\n\u001b[0;32m--> 740\u001b[0m inertia \u001b[38;5;241m=\u001b[39m _inertia(X, sample_weight, centers, labels, n_threads)\n\u001b[1;32m    742\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m labels, inertia, centers, i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 75\u001b[0m\n\u001b[1;32m     73\u001b[0m     labels \u001b[38;5;241m=\u001b[39m kmeans\u001b[38;5;241m.\u001b[39mlabels_\n\u001b[1;32m     74\u001b[0m     silhouette_scores\u001b[38;5;241m.\u001b[39mappend(silhouette_score(sample_space, labels))\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConvergenceWarning \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(silhouette_scores)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0.7\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ConvergenceWarning' is not defined"
     ]
    }
   ],
=======
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
>>>>>>> f76b1bccfaaf61af4dedfdd10e50b8ef97203553
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "prints = False\n",
    "\n",
    "\n",
    "#ignore warnings temporarily for better readability\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def sample(upper_limit1, amount_of_samples1, data1):\n",
    "    rand_numbs = random.sample(range(0, upper_limit1), amount_of_samples1)\n",
    "    sample_space = [list(data1[i]) for i in rand_numbs]\n",
    "    return sample_space\n",
    "\n",
    "def round_list(lst, decimal_places=0):\n",
    "    rounded_list = [round(element, decimal_places) for element in lst]\n",
    "    return rounded_list\n",
    "\n",
    "ext_data_points = {}\n",
    "clusterinfo = {}\n",
    "count = 0\n",
    "for city in data_points:\n",
    "    count = count + 1\n",
    "    max_expected_clusters = 0\n",
    "    labeling = {}\n",
    "    data = data_points[city]\n",
    "    #limit the sample space\n",
    "    datasize = len(data)\n",
    "    upper_limit = datasize\n",
    "    if datasize < 1000:\n",
    "        amount_of_samples = min(datasize, 100)\n",
    "        max_expected_clusters = round(float(amount_of_samples)**(1/2))\n",
    "    elif datasize < 10000:\n",
    "        amount_of_samples = round(datasize/10)\n",
    "        max_expected_clusters = round(float(amount_of_samples)**(1/2))\n",
    "\n",
    "    else:\n",
    "        amount_of_samples = round(datasize/100)\n",
    "        max_expected_clusters = round(float(amount_of_samples)**(1/2))\n",
    "\n",
    "    sample_space = []\n",
    "    \n",
    "    #check if all the samplepoint    print(amount_of_samples)\n",
    "    #are the same and if they are try again 5 times\n",
    "    #if still the same pass the knowledge on\n",
    "    for _ in range(5):\n",
    "        one_point_marker = True\n",
    "        sample_space = sample(upper_limit, amount_of_samples, data)\n",
    "        if all(x == data[0] for x in sample_space) != True:\n",
    "            one_point_marker = False\n",
    "            break\n",
    "            \n",
    "    # Calculate silhouette scores for different values of k using samples \n",
    "    silhouette_scores = []\n",
    "    #limit expected amount of clusters proportional to the amount of datapoints we can maybe say that for every 2% of dataset there may be a cluster existing\n",
    "    \n",
    "    K_range = range(2, max_expected_clusters)\n",
    "\n",
    "    if(prints==True): print(str(count) + \"/\"+str(len(data_points)) + \" trying to find \" + str(max_expected_clusters) + \" clusters for subspace: \" + city + \" in \" + str(amount_of_samples) + \" samples with datasize of \" + str(datasize))\n",
    "\n",
    "    \n",
    "    if (one_point_marker == False) and (max_expected_clusters > 2):\n",
    "        #loop through possible amount of clusters and stop for first silhouette with score >0.7\n",
    "        for k in K_range:\n",
    "            try:\n",
    "                scaler = MinMaxScaler()\n",
    "                sample_space = scaler.fit_transform(sample_space)\n",
    "                kmeans = KMeans(n_clusters=k)\n",
    "                kmeans.fit(sample_space)\n",
    "                labels = kmeans.labels_\n",
    "                silhouette_scores.append(silhouette_score(sample_space, labels))\n",
    "            except ConvergenceWarning as e:\n",
    "                break\n",
    "            if max(silhouette_scores) > 0.7:\n",
    "                break\n",
    "        \n",
    "        #determine k using the sampled space\n",
    "        max_ss = max(silhouette_scores)\n",
    "        if max_ss > 0.6:\n",
    "            k = silhouette_scores.index(max_ss) + 2\n",
    "        else:\n",
    "            k = 1\n",
    "            \n",
    "    else:\n",
    "        k = 1\n",
    "    if(prints==True): print(\"found \" + str(k) + \" clusters. Now perform clustering on the data:\")\n",
    "    #now use kmeans to find the labels of the points for the entire set\n",
    "    scaler = MinMaxScaler()\n",
    "    data = scaler.fit_transform(data)\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(data)\n",
    "\n",
    "    if(prints==True): print(\"done\")\n",
    "    if(prints==True): print(\"building map with labels and cluster info\")\n",
    "    #build the extended data points set which also contain the labels of the points\n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    for index, point in enumerate(data_points[city]):\n",
    "        labeling[tuple(point)] = labels[index]\n",
    "    ext_data_points[city] = labeling\n",
    "    \n",
    "    #build a dataset which contains info about each cluster\n",
    "    for i in range(k):\n",
    "        temp = {}\n",
    "        oneclusterdata = data[labels==i]\n",
    "        temp[\"count\"] = (len(oneclusterdata))\n",
    "        kmeans1 = KMeans(n_clusters=1)\n",
    "        kmeans1.fit(oneclusterdata)\n",
    "        temp[\"inertia\"] = (kmeans1.inertia_)\n",
    "        temp[\"centroid\"] = tuple(round_list(scaler.inverse_transform(kmeans.cluster_centers_)[i]))\n",
    "        clusterinfo[city + \"-\" + str(i)] = temp\n",
    "if(prints==True): print(\"found \" + str(len(clusterinfo)) + \" clusters\")\n",
    "\n",
    "#if(prints==True): print(ext_data_points)\n",
    "#if(prints==True): print(clusterinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now build the new, transformed dataset out of the old dataset and the gathered data\n",
    "#the data we gathered are the mapping tool (dim_count) and the ext_data_points\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "dataset = []\n",
    "dim_map = dim_count\n",
    "clust_map = ext_data_points\n",
    "\n",
    "for route_info in data:\n",
    "    conv_route = []\n",
    "    cont_trips = []\n",
    "    \n",
    "    \n",
    "    for trip_info in route_info[\"route\"]:\n",
    "        conn_name = trip_info[\"from\"] + \"-\" + trip_info[\"to\"]\n",
    "        \n",
    "        temp_point = [0] * len(dim_map[conn_name])\n",
    "        \n",
    "        for merch in trip_info[\"merchandise\"]:\n",
    "            index = dim_map[conn_name].index(merch)\n",
    "            temp_point[index] = trip_info[\"merchandise\"][merch]\n",
    "        cluster = clust_map[conn_name][tuple(temp_point)]\n",
    "        trip_name = conn_name + \"-\" + str(cluster)\n",
    "        cont_trips.append(trip_name)\n",
    "        \n",
    "    conv_route.append(route_info[\"id\"])\n",
    "    conv_route.append(route_info[\"driver\"])\n",
    "    conv_route.append(route_info[\"sroute\"])\n",
    "    conv_route.append(tuple(cont_trips))\n",
    "    dataset.append(tuple(conv_route))\n",
    "    \n",
    "dataset = tuple(dataset)\n",
    "#for x in dataset:\n",
    "#    if(prints==True): print(x)\n",
    "\n",
    "\n",
    "#the structure is as follows:\n",
    "#(id, driver, sroute, (city1-city2-clusternumber))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the singletons not frequent enough, depending on a certain thresehold. \n",
    "#In this case, I've chosen 100. \n",
    "#The function outputs a tuple with the frequent items and not frequently enough items.\n",
    "def prune(X, thresehold):\n",
    "    reduce = []\n",
    "    for i in X:\n",
    "        if X[i] < thresehold:\n",
    "            reduce.append(((i,), X[i]))\n",
    "    for i in reduce:\n",
    "        X.pop(i[0][0])\n",
    "    return (X, reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function that, given two trips, outputs wether they could be chained or not,\n",
    "#for example, if the first trip ends in Milano, it tells you wether the second one\n",
    "#starts in Milano too. In this way, we only create \"possible couples\" with the next function.\n",
    "def match(x, y):\n",
    "    w1 = x\n",
    "    l1 = len(w1)\n",
    "    w1 = w1[:l1-2]\n",
    "    index1 = w1.index('-')\n",
    "    w1 = w1[index1+1:]\n",
    "    \n",
    "    w2 = y\n",
    "    l2 = len(w2)\n",
    "    w2 = w2[:l2-2]\n",
    "    index2 = w2.index('-')\n",
    "    w2 = w2[:index2]\n",
    "    \n",
    "    return w1 == w2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the above function we create couples.It also gives us a list \n",
    "#of the singletons that we could not extend, and a list of the ones we could.\n",
    "#We need the latter in case the couples don't pass the prune in the next step;\n",
    "#in that case, we would go back one step and offer the routes that created that couple.\n",
    "\n",
    "def couples(X):\n",
    "    nextcandidates = []\n",
    "    extend = []\n",
    "    for i in X:\n",
    "        extend.append(((i,), X[i]))\n",
    "    lista = []\n",
    "    for i in X:\n",
    "        for j in X:\n",
    "            if match(i, j):\n",
    "                if ((i,), X[i]) in extend:\n",
    "                    extend.remove(((i,), X[i]))\n",
    "                if ((j,), X[j]) in extend:\n",
    "                    extend.remove(((j,), X[j]))\n",
    "                    \n",
    "                if (i, j) not in lista:\n",
    "                    lista.append((i, j))\n",
    "                if ((i,), X[i]) not in nextcandidates:\n",
    "                        nextcandidates.append(((i,), X[i]))\n",
    "                if ((j,), X[j]) not in nextcandidates:\n",
    "                        nextcandidates.append(((j,), X[j]))\n",
    "    return (lista, extend, nextcandidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function tells us wether a given sequence is a subsequence of a longer one.\n",
    "#basically we are certifying if a given subroute is part of a bigger route in the dataset.\n",
    "\n",
    "def subsequence(m, S):\n",
    "    l = len(m)\n",
    "    L = len(S)\n",
    "    for i in range(0, L-l+1):\n",
    "        if S[i:i+l] == m:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#given possible frequent tuples it creates a dictionary with the occurrences\n",
    "#of such tuples as subsequences in the dataset.\n",
    "\n",
    "def find(candidates, X):\n",
    "    freq = {}\n",
    "    for i in candidates:\n",
    "        for j in X:\n",
    "            if subsequence(i, j[3]):\n",
    "                if i in freq:\n",
    "                    freq[i] = freq[i]+1\n",
    "                else:\n",
    "                    freq[i] = 1\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function takes a list of tuples of the same length and combines them\n",
    "#to create new tuples of length + 1. Furthermore, it only creates \"logical\"\n",
    "#tuples. That is, it won't combine (1,2,3) with (1,2,5) but it will combine \n",
    "#(1,2,3) with (2,3,4) to create (1,2,3,4). It also gives as a list of tuples\n",
    "#we could not extend, and the ones we could, for analogous reasons as in the \n",
    "#case of the function couples.\n",
    "\n",
    "def combine(X):\n",
    "    extend = []\n",
    "    for i in X:\n",
    "        extend.append((i, X[i]))\n",
    "    nextcandidates = []\n",
    "    lista = []\n",
    "    if len(X) != 0:\n",
    "        for i in X:\n",
    "            l = len(i)\n",
    "            break\n",
    "        for tuple1 in X:\n",
    "            for tuple2 in X:\n",
    "                if tuple2[:l-1] == tuple1[1:]:\n",
    "                    newtuple = tuple1 + (tuple2[l-1],)\n",
    "                    if (tuple1, X[tuple1]) in extend:\n",
    "                        extend.remove((tuple1, X[tuple1]))\n",
    "                    if (tuple2, X[tuple2]) in extend:\n",
    "                        extend.remove((tuple2, X[tuple2]))\n",
    "                    if newtuple not in lista:\n",
    "                        lista.append(newtuple)\n",
    "                    if (tuple1, X[tuple1]) not in nextcandidates:\n",
    "                        nextcandidates.append((tuple1, X[tuple1]))\n",
    "                    if (tuple2, X[tuple2]) not in nextcandidates:\n",
    "                        nextcandidates.append((tuple2, X[tuple2]))\n",
    "    return (lista, extend, nextcandidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It takes a trip-clusters and converts it into an actual trip.\n",
    "\n",
    "def cluster_to_info(x,clusterinfo,dim_map):\n",
    "    \n",
    "    w1 = x\n",
    "    l1 = len(w1)\n",
    "    w1 = w1[:l1-2]\n",
    "    index1 = w1.index('-')\n",
    "    w1 = w1[index1+1:]\n",
    "    \n",
    "    w2 = x[:l1-2]\n",
    "    index2=w2.index('-')\n",
    "    w2 = w2[:index2]\n",
    "    \n",
    "    trip = {}\n",
    "    trip['from'] = w2\n",
    "    \n",
    "    \n",
    "    info = clusterinfo[x]['centroid']\n",
    "    l = len(x) - 2\n",
    "    merch = dim_map[x[:l]]\n",
    "    s = {}\n",
    "    for i in range(len(merch)):\n",
    "         if info[i] != 0:\n",
    "            s[merch[i]] = info[i]\n",
    "            \n",
    "    trip['merchandise'] = s\n",
    "    trip['to'] = w1\n",
    "    \n",
    "    return trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It takes all of the cluster-routes and convert them into actual routes.\n",
    "\n",
    "def offeroutes(OFFER):\n",
<<<<<<< HEAD
    "    \n",
    "    lista=[]\n",
    "    H=len(OFFER)\n",
    "    for i in range(H):\n",
    "        ruta={}\n",
    "        ruta['id']='s'+str(i+1)\n",
    "        TRIPS=[]\n",
=======
    "    lista = []\n",
    "    L = len(OFFER)\n",
    "    for i in range(L):\n",
    "        ruta = {}\n",
    "        ruta['id'] = 's'+str(i+1)\n",
    "        TRIPS = []\n",
>>>>>>> f76b1bccfaaf61af4dedfdd10e50b8ef97203553
    "        for j in OFFER[i][0]:\n",
    "            trip = cluster_to_info(j, clusterinfo, dim_map)\n",
    "            TRIPS.append(trip)\n",
<<<<<<< HEAD
    "        ruta['route']=TRIPS\n",
    "        lista.append((ruta,OFFER[i][1]))\n",
    "    L=[]\n",
    "    for i in lista:\n",
    "        L.append(i[1])\n",
    "    L.sort()\n",
    "    L=L[::-1]\n",
    "    Lista=[]\n",
    "    for i in L:\n",
    "        for j in lista:\n",
    "            if i==j[1]:\n",
    "                Lista.append(j[0)\n",
    "                lista.remove(j)\n",
    "                break   \n",
    "    for i in range(H):\n",
    "        Lista[i][0]['id']='s'+str(i+1)\n",
    "    return Lista\n",
    "            \n",
    "        \n",
    "    "
=======
    "        ruta['route'] = TRIPS\n",
    "        lista.append(ruta)\n",
    "    return lista"
>>>>>>> f76b1bccfaaf61af4dedfdd10e50b8ef97203553
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FREQUENT_ITEMS(dataset,thresehold,NS):\n",
    "    OFFER=[]\n",
    "    \n",
=======
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FREQUENT_ITEMS(dataset, thresehold):\n",
    "    OFFER = []\n",
>>>>>>> f76b1bccfaaf61af4dedfdd10e50b8ef97203553
    "        \n",
    "    freq = {}\n",
    "    for x in dataset:\n",
    "        for i in x[3]:\n",
    "            if i not in freq:\n",
    "                freq[i] = 1\n",
    "            else:\n",
    "                freq[i] = freq[i] + 1\n",
    "    \n",
    "    #we can also use clusterinfo to get freq\n",
    "    #freq = {}\n",
    "    #for i in clusterinfo:\n",
    "    #    freq[i] = clusterinfo[i]['count']\n",
    "    \n",
    "    \n",
    "    S = prune(freq,thresehold)[0]\n",
    "    (candidates, extend, nextcandidates) = couples(S)\n",
    "    OFFER = OFFER + extend\n",
    "    \n",
    "    \n",
    "    while len(candidates) != 0:\n",
    "        new = find(candidates, dataset)\n",
    "        (S, H) = prune(new, thresehold)\n",
    "        count=0\n",
    "        for j in nextcandidates:\n",
    "            for i in H:\n",
    "                if subsequence(j[0], i[0]):\n",
    "                    OFFER = OFFER + [j]\n",
    "                    count = count+1\n",
    "            if count == 0:\n",
    "                for i in S:\n",
    "                    if subsequence(j[0], i):\n",
    "                        count = count+1\n",
    "            if count == 0:\n",
    "                OFFER = OFFER + [j]\n",
    "                \n",
<<<<<<< HEAD
    "        (candidates,extend,nextcandidates)=combine(S)\n",
    "        OFFER = OFFER+ extend\n",
    "        \n",
    "    return offeroutes(OFFER)\n",
    "        \n",
    "\n",
    "    "
=======
    "        (candidates, extend, nextcandidates) = combine(S)\n",
    "        OFFER = OFFER + extend\n",
    "    return offeroutes(OFFER)"
>>>>>>> f76b1bccfaaf61af4dedfdd10e50b8ef97203553
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OFFER_ROUTES(dataset,NS):\n",
    "    n=0\n",
    "    OFFER= FREQUENT_ITEMS(dataset,len(dataset)/(2**n))\n",
    "    while len(OFFER)<NS:\n",
    "        n=n+1\n",
    "        OFFER=FREQUENT_ITEMS(dataset,len(dataset)/(2**n))\n",
    "    return (OFFER[:NS])\n",
    "    "
=======
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 's1',\n",
       "  'route': [{'from': 'Modena',\n",
       "    'merchandise': {'Water': 8.0,\n",
       "     'Potatoes': 3.9,\n",
       "     'Butter': 14.6,\n",
       "     'Honey': 27.0,\n",
       "     'Pasta': 4.8},\n",
       "    'to': 'Genova'}]},\n",
       " {'id': 's2',\n",
       "  'route': [{'from': 'Genova',\n",
       "    'merchandise': {'Water': 0.2,\n",
       "     'Milk': 0.1,\n",
       "     'Chocolate': 0.2,\n",
       "     'Rice': 0.2,\n",
       "     'Pens': 0.1,\n",
       "     'Potatoes': 0.2,\n",
       "     'Butter': 0.1,\n",
       "     'Apples': 3.4,\n",
       "     'Honey': 0.1,\n",
       "     'Carrots': 0.2,\n",
       "     'Bananas': 28.0,\n",
       "     'Fish': 0.2,\n",
       "     'Bread': 0.2,\n",
       "     'Pasta': 0.2,\n",
       "     'Yogurt': 0.2,\n",
       "     'Tea': 0.2,\n",
       "     'Tomatoes': 0.2,\n",
       "     'Meat': 10.5},\n",
       "    'to': 'Bolzano'}]},\n",
       " {'id': 's3',\n",
       "  'route': [{'from': 'Lecce',\n",
       "    'merchandise': {'Chocolate': 25.0,\n",
       "     'Potatoes': 15.0,\n",
       "     'Yogurt': 11.0,\n",
       "     'Tomatoes': 11.3,\n",
       "     'Meat': 9.3},\n",
       "    'to': 'Padua'},\n",
       "   {'from': 'Padua',\n",
       "    'merchandise': {'Cheese': 1.1,\n",
       "     'Chocolate': 7.5,\n",
       "     'Potatoes': 5.0,\n",
       "     'Beer': 18.2,\n",
       "     'Honey': 1.2,\n",
       "     'Fish': 19.2,\n",
       "     'Meat': 16.5},\n",
       "    'to': 'Genova'}]},\n",
       " {'id': 's4',\n",
       "  'route': [{'from': 'Monza',\n",
       "    'merchandise': {'Water': 18.4,\n",
       "     'Cheese': 1.1,\n",
       "     'Chocolate': 5.6,\n",
       "     'Potatoes': 8.3,\n",
       "     'Beer': 1.1,\n",
       "     'Butter': 24.8,\n",
       "     'Fish': 4.8,\n",
       "     'Honey': 1.1,\n",
       "     'Meat': 23.3},\n",
       "    'to': 'Genova'},\n",
       "   {'from': 'Genova',\n",
       "    'merchandise': {'Cheese': 1.9,\n",
       "     'Chocolate': 0.2,\n",
       "     'Beer': 0.1,\n",
       "     'Butter': 19.7,\n",
       "     'Apples': 19.7,\n",
       "     'Honey': 0.2,\n",
       "     'Bananas': 10.7,\n",
       "     'Fish': 0.1,\n",
       "     'Tea': 4.9},\n",
       "    'to': 'Bolzano'}]},\n",
       " {'id': 's5',\n",
       "  'route': [{'from': 'Bolzano',\n",
       "    'merchandise': {'Water': 16.2,\n",
       "     'Cheese': 26.0,\n",
       "     'Chocolate': 1.1,\n",
       "     'Potatoes': 22.5,\n",
       "     'Beer': 0.8,\n",
       "     'Honey': 16.5,\n",
       "     'Pasta': 14.7},\n",
       "    'to': 'Pisa'},\n",
       "   {'from': 'Pisa',\n",
       "    'merchandise': {'Cheese': 0.2,\n",
       "     'Pens': 24.0,\n",
       "     'Chocolate': 0.3,\n",
       "     'Beer': 2.9,\n",
       "     'Butter': 12.7,\n",
       "     'Honey': 0.3,\n",
       "     'Bananas': 3.8,\n",
       "     'Bread': 8.0},\n",
       "    'to': 'Bologna'}]},\n",
       " {'id': 's6',\n",
       "  'route': [{'from': 'Udine',\n",
       "    'merchandise': {'Water': 0.2,\n",
       "     'Cheese': 1.2,\n",
       "     'Milk': 0.2,\n",
       "     'Chocolate': 1.4,\n",
       "     'Rice': 27.4,\n",
       "     'Pens': 0.2,\n",
       "     'Potatoes': 0.2,\n",
       "     'Beer': 1.1,\n",
       "     'Butter': 3.4,\n",
       "     'Carrots': 0.2,\n",
       "     'Apples': 0.2,\n",
       "     'Fish': 0.2,\n",
       "     'Bananas': 0.1,\n",
       "     'Honey': 27.4,\n",
       "     'Bread': 0.2,\n",
       "     'Pasta': 0.1,\n",
       "     'Yogurt': 0.1,\n",
       "     'Tea': 0.2,\n",
       "     'Tomatoes': 0.2,\n",
       "     'Meat': 0.2},\n",
       "    'to': 'Verona'},\n",
       "   {'from': 'Verona',\n",
       "    'merchandise': {'Water': 0.1,\n",
       "     'Cheese': 1.0,\n",
       "     'Milk': 0.1,\n",
       "     'Chocolate': 7.7,\n",
       "     'Pens': 22.0,\n",
       "     'Rice': 0.1,\n",
       "     'Potatoes': 0.2,\n",
       "     'Beer': 0.9,\n",
       "     'Butter': 20.2,\n",
       "     'Honey': 17.7,\n",
       "     'Apples': 0.1,\n",
       "     'Fish': 0.1,\n",
       "     'Bananas': 0.1,\n",
       "     'Carrots': 0.1,\n",
       "     'Bread': 0.1,\n",
       "     'Pasta': 0.1,\n",
       "     'Yogurt': 0.1,\n",
       "     'Tea': 0.1,\n",
       "     'Tomatoes': 0.1,\n",
       "     'Meat': 0.1},\n",
       "    'to': 'Siena'}]},\n",
       " {'id': 's7',\n",
       "  'route': [{'from': 'Verona',\n",
       "    'merchandise': {'Beer': 12.8,\n",
       "     'Apples': 3.8,\n",
       "     'Honey': 10.8,\n",
       "     'Yogurt': 25.0,\n",
       "     'Tomatoes': 6.2,\n",
       "     'Meat': 27.0},\n",
       "    'to': 'Rome'},\n",
       "   {'from': 'Rome',\n",
       "    'merchandise': {'Water': 0.1,\n",
       "     'Cheese': 3.0,\n",
       "     'Milk': 0.1,\n",
       "     'Pens': 0.1,\n",
       "     'Chocolate': 0.9,\n",
       "     'Rice': 0.2,\n",
       "     'Potatoes': 0.2,\n",
       "     'Beer': 7.0,\n",
       "     'Butter': 0.2,\n",
       "     'Apples': 10.0,\n",
       "     'Carrots': 0.1,\n",
       "     'Honey': 0.8,\n",
       "     'Bananas': 0.1,\n",
       "     'Fish': 0.1,\n",
       "     'Bread': 0.2,\n",
       "     'Pasta': 23.0,\n",
       "     'Yogurt': 0.1,\n",
       "     'Tea': 0.1,\n",
       "     'Tomatoes': 0.2,\n",
       "     'Meat': 0.1},\n",
       "    'to': 'Monza'}]},\n",
       " {'id': 's8',\n",
       "  'route': [{'from': 'Torino',\n",
       "    'merchandise': {'Water': 0.1,\n",
       "     'Cheese': 1.0,\n",
       "     'Milk': 0.1,\n",
       "     'Pens': 0.2,\n",
       "     'Rice': 0.1,\n",
       "     'Potatoes': 0.1,\n",
       "     'Beer': 1.1,\n",
       "     'Butter': 27.0,\n",
       "     'Fish': 0.1,\n",
       "     'Apples': 0.1,\n",
       "     'Honey': 11.6,\n",
       "     'Bananas': 0.1,\n",
       "     'Carrots': 0.2,\n",
       "     'Bread': 0.2,\n",
       "     'Pasta': 0.1,\n",
       "     'Yogurt': 0.2,\n",
       "     'Tea': 30.0,\n",
       "     'Tomatoes': 0.2,\n",
       "     'Meat': 0.1},\n",
       "    'to': 'Padua'},\n",
       "   {'from': 'Padua',\n",
       "    'merchandise': {'Bananas': 7.0,\n",
       "     'Apples': 19.4,\n",
       "     'Pasta': 26.0,\n",
       "     'Tea': 23.0,\n",
       "     'Meat': 30.0},\n",
       "    'to': 'Venezia'}]},\n",
       " {'id': 's9',\n",
       "  'route': [{'from': 'Torino',\n",
       "    'merchandise': {'Cheese': 0.9,\n",
       "     'Rice': 24.0,\n",
       "     'Chocolate': 1.1,\n",
       "     'Beer': 23.9,\n",
       "     'Carrots': 22.4,\n",
       "     'Honey': 16.3,\n",
       "     'Tomatoes': 20.6,\n",
       "     'Meat': 7.7},\n",
       "    'to': 'Bologna'},\n",
       "   {'from': 'Bologna',\n",
       "    'merchandise': {'Water': 0.1,\n",
       "     'Cheese': 0.1,\n",
       "     'Milk': 0.1,\n",
       "     'Chocolate': 0.2,\n",
       "     'Rice': 25.0,\n",
       "     'Pens': 0.1,\n",
       "     'Potatoes': 0.1,\n",
       "     'Beer': 0.1,\n",
       "     'Butter': 0.1,\n",
       "     'Apples': 0.2,\n",
       "     'Honey': 0.2,\n",
       "     'Carrots': 0.2,\n",
       "     'Bananas': 0.1,\n",
       "     'Fish': 0.1,\n",
       "     'Bread': 0.1,\n",
       "     'Pasta': 0.1,\n",
       "     'Yogurt': 0.1,\n",
       "     'Tea': 6.0,\n",
       "     'Tomatoes': 13.9,\n",
       "     'Meat': 16.0},\n",
       "    'to': 'Bolzano'}]},\n",
       " {'id': 's10',\n",
       "  'route': [{'from': 'Siena',\n",
       "    'merchandise': {'Cheese': 15.3,\n",
       "     'Milk': 12.0,\n",
       "     'Rice': 3.9,\n",
       "     'Potatoes': 15.4,\n",
       "     'Beer': 16.3,\n",
       "     'Honey': 15.9},\n",
       "    'to': 'Udine'},\n",
       "   {'from': 'Udine',\n",
       "    'merchandise': {'Water': 0.1,\n",
       "     'Cheese': 1.0,\n",
       "     'Milk': 0.1,\n",
       "     'Chocolate': 1.4,\n",
       "     'Rice': 0.1,\n",
       "     'Pens': 0.1,\n",
       "     'Potatoes': 13.0,\n",
       "     'Beer': 1.1,\n",
       "     'Butter': 9.6,\n",
       "     'Carrots': 0.2,\n",
       "     'Apples': 0.3,\n",
       "     'Fish': 0.1,\n",
       "     'Bananas': 0.2,\n",
       "     'Honey': 1.4,\n",
       "     'Bread': 0.3,\n",
       "     'Pasta': 22.5,\n",
       "     'Yogurt': 7.8,\n",
       "     'Tea': 0.1,\n",
       "     'Tomatoes': 0.2,\n",
       "     'Meat': 0.2},\n",
       "    'to': 'Verona'}]},\n",
       " {'id': 's11',\n",
       "  'route': [{'from': 'Napoli',\n",
       "    'merchandise': {'Cheese': 26.0,\n",
       "     'Chocolate': 29.0,\n",
       "     'Rice': 23.0,\n",
       "     'Potatoes': 13.7,\n",
       "     'Beer': 1.4,\n",
       "     'Bananas': 4.4,\n",
       "     'Honey': 17.5},\n",
       "    'to': 'Lecce'},\n",
       "   {'from': 'Lecce',\n",
       "    'merchandise': {'Water': 0.1,\n",
       "     'Cheese': 30.0,\n",
       "     'Milk': 0.1,\n",
       "     'Chocolate': 9.0,\n",
       "     'Pens': 0.1,\n",
       "     'Rice': 0.1,\n",
       "     'Potatoes': 0.1,\n",
       "     'Butter': 29.0,\n",
       "     'Apples': 0.1,\n",
       "     'Fish': 0.1,\n",
       "     'Bananas': 0.1,\n",
       "     'Carrots': 0.1,\n",
       "     'Bread': 0.2,\n",
       "     'Pasta': 0.2,\n",
       "     'Yogurt': 0.2,\n",
       "     'Tea': 25.0,\n",
       "     'Tomatoes': 0.1,\n",
       "     'Meat': 0.1},\n",
       "    'to': 'Modena'}]},\n",
       " {'id': 's12',\n",
       "  'route': [{'from': 'Siena',\n",
       "    'merchandise': {'Water': 0.1,\n",
       "     'Cheese': 0.9,\n",
       "     'Milk': 0.1,\n",
       "     'Pens': 27.5,\n",
       "     'Chocolate': 1.3,\n",
       "     'Rice': 12.3,\n",
       "     'Potatoes': 0.1,\n",
       "     'Beer': 0.9,\n",
       "     'Butter': 0.1,\n",
       "     'Fish': 0.1,\n",
       "     'Apples': 0.1,\n",
       "     'Honey': 1.3,\n",
       "     'Bananas': 0.1,\n",
       "     'Carrots': 0.1,\n",
       "     'Bread': 0.1,\n",
       "     'Pasta': 0.1,\n",
       "     'Yogurt': 9.7,\n",
       "     'Tea': 0.1,\n",
       "     'Tomatoes': 0.1,\n",
       "     'Meat': 18.5},\n",
       "    'to': 'Rome'},\n",
       "   {'from': 'Rome',\n",
       "    'merchandise': {'Cheese': 26.0,\n",
       "     'Milk': 9.9,\n",
       "     'Pens': 23.4,\n",
       "     'Chocolate': 1.1,\n",
       "     'Potatoes': 13.8,\n",
       "     'Beer': 0.7,\n",
       "     'Honey': 1.1,\n",
       "     'Tea': 19.1},\n",
       "    'to': 'Bolzano'},\n",
       "   {'from': 'Bolzano',\n",
       "    'merchandise': {'Water': 0.1,\n",
       "     'Milk': 0.1,\n",
       "     'Rice': 26.0,\n",
       "     'Potatoes': 0.1,\n",
       "     'Butter': 0.1,\n",
       "     'Apples': 0.1,\n",
       "     'Honey': 28.0,\n",
       "     'Fish': 0.1,\n",
       "     'Bananas': 0.2,\n",
       "     'Carrots': 0.1,\n",
       "     'Bread': 0.1,\n",
       "     'Yogurt': 9.0,\n",
       "     'Tomatoes': 0.1,\n",
       "     'Meat': 0.1},\n",
       "    'to': 'Bergamo'}]},\n",
       " {'id': 's13',\n",
       "  'route': [{'from': 'Trento',\n",
       "    'merchandise': {'Cheese': 0.1,\n",
       "     'Beer': 1.9,\n",
       "     'Apples': 16.8,\n",
       "     'Honey': 16.7,\n",
       "     'Bread': 2.8,\n",
       "     'Tomatoes': 16.3,\n",
       "     'Meat': 16.1},\n",
       "    'to': 'Monza'},\n",
       "   {'from': 'Monza',\n",
       "    'merchandise': {'Cheese': 0.9,\n",
       "     'Milk': 17.7,\n",
       "     'Chocolate': 1.0,\n",
       "     'Rice': 13.8,\n",
       "     'Potatoes': 21.5,\n",
       "     'Beer': 0.8,\n",
       "     'Honey': 1.0,\n",
       "     'Yogurt': 5.9,\n",
       "     'Tea': 6.8},\n",
       "    'to': 'Genova'},\n",
       "   {'from': 'Genova',\n",
       "    'merchandise': {'Cheese': 0.3,\n",
       "     'Beer': 0.2,\n",
       "     'Butter': 11.7,\n",
       "     'Apples': 10.0,\n",
       "     'Honey': 0.1,\n",
       "     'Carrots': 15.4,\n",
       "     'Fish': 7.0,\n",
       "     'Bread': 11.9,\n",
       "     'Tea': 5.6},\n",
       "    'to': 'Bolzano'}]},\n",
       " {'id': 's14',\n",
       "  'route': [{'from': 'Venezia',\n",
       "    'merchandise': {'Water': 0.2,\n",
       "     'Cheese': 1.1,\n",
       "     'Milk': 0.2,\n",
       "     'Pens': 20.8,\n",
       "     'Rice': 0.2,\n",
       "     'Chocolate': 0.1,\n",
       "     'Potatoes': 0.2,\n",
       "     'Beer': 1.2,\n",
       "     'Butter': 0.2,\n",
       "     'Apples': 14.8,\n",
       "     'Fish': 0.2,\n",
       "     'Honey': 0.1,\n",
       "     'Bananas': 0.2,\n",
       "     'Carrots': 0.2,\n",
       "     'Bread': 0.3,\n",
       "     'Pasta': 0.2,\n",
       "     'Yogurt': 21.7,\n",
       "     'Tea': 0.2,\n",
       "     'Tomatoes': 0.2,\n",
       "     'Meat': 0.2},\n",
       "    'to': 'Verona'},\n",
       "   {'from': 'Verona',\n",
       "    'merchandise': {'Water': 0.1,\n",
       "     'Cheese': 0.8,\n",
       "     'Milk': 0.1,\n",
       "     'Pens': 0.1,\n",
       "     'Rice': 0.1,\n",
       "     'Chocolate': 16.9,\n",
       "     'Potatoes': 0.1,\n",
       "     'Beer': 0.9,\n",
       "     'Butter': 0.1,\n",
       "     'Carrots': 0.1,\n",
       "     'Apples': 0.1,\n",
       "     'Honey': 16.1,\n",
       "     'Bananas': 0.1,\n",
       "     'Fish': 0.1,\n",
       "     'Bread': 0.1,\n",
       "     'Pasta': 0.1,\n",
       "     'Yogurt': 23.9,\n",
       "     'Tea': 0.1,\n",
       "     'Tomatoes': 0.1,\n",
       "     'Meat': 12.3},\n",
       "    'to': 'Bergamo'},\n",
       "   {'from': 'Bergamo',\n",
       "    'merchandise': {'Cheese': 12.9,\n",
       "     'Chocolate': 0.1,\n",
       "     'Potatoes': 7.8,\n",
       "     'Butter': 18.4,\n",
       "     'Honey': 0.1,\n",
       "     'Apples': 16.4,\n",
       "     'Tea': 6.9},\n",
       "    'to': 'Siena'}]},\n",
       " {'id': 's15',\n",
       "  'route': [{'from': 'Verona',\n",
       "    'merchandise': {'Water': 0.1,\n",
       "     'Cheese': 0.8,\n",
       "     'Milk': 0.1,\n",
       "     'Pens': 0.1,\n",
       "     'Rice': 0.1,\n",
       "     'Chocolate': 16.9,\n",
       "     'Potatoes': 0.1,\n",
       "     'Beer': 0.9,\n",
       "     'Butter': 0.1,\n",
       "     'Carrots': 0.1,\n",
       "     'Apples': 0.1,\n",
       "     'Honey': 16.1,\n",
       "     'Bananas': 0.1,\n",
       "     'Fish': 0.1,\n",
       "     'Bread': 0.1,\n",
       "     'Pasta': 0.1,\n",
       "     'Yogurt': 23.9,\n",
       "     'Tea': 0.1,\n",
       "     'Tomatoes': 0.1,\n",
       "     'Meat': 12.3},\n",
       "    'to': 'Bergamo'},\n",
       "   {'from': 'Bergamo',\n",
       "    'merchandise': {'Cheese': 12.9,\n",
       "     'Chocolate': 0.1,\n",
       "     'Potatoes': 7.8,\n",
       "     'Butter': 18.4,\n",
       "     'Honey': 0.1,\n",
       "     'Apples': 16.4,\n",
       "     'Tea': 6.9},\n",
       "    'to': 'Siena'},\n",
       "   {'from': 'Siena',\n",
       "    'merchandise': {'Water': 0.1,\n",
       "     'Cheese': 0.1,\n",
       "     'Chocolate': 0.3,\n",
       "     'Pens': 7.0,\n",
       "     'Rice': 0.1,\n",
       "     'Potatoes': 0.1,\n",
       "     'Beer': 0.1,\n",
       "     'Butter': 0.1,\n",
       "     'Carrots': 0.2,\n",
       "     'Fish': 0.2,\n",
       "     'Honey': 0.2,\n",
       "     'Bananas': 0.1,\n",
       "     'Apples': 4.0,\n",
       "     'Bread': 0.1,\n",
       "     'Pasta': 0.1,\n",
       "     'Yogurt': 9.0,\n",
       "     'Tea': 0.1,\n",
       "     'Tomatoes': 0.1,\n",
       "     'Meat': 0.1},\n",
       "    'to': 'Bolzano'}]}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OFFER = FREQUENT_ITEMS(dataset, len(dataset)/40)\n",
    "OFFER"
>>>>>>> f76b1bccfaaf61af4dedfdd10e50b8ef97203553
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
=======
   "display_name": "base",
   "language": "python",
   "name": "python3"
>>>>>>> f76b1bccfaaf61af4dedfdd10e50b8ef97203553
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.11.5"
=======
   "version": "3.8.10"
>>>>>>> f76b1bccfaaf61af4dedfdd10e50b8ef97203553
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
