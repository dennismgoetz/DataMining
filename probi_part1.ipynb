{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"actual_routes.json\"\n",
    "\n",
    "#import data\n",
    "import json\n",
    "print(\"loading data into memory\")\n",
    "with open(filename, 'r') as file:\n",
    "    data = json.load(file)\n",
    "print(\"loaded \"+str(len(data))+\" routes into memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the amount of dimensions (possible merch) for all the possible \n",
    "#connections between cities \n",
    "print(\"determine subspaces and its dimensions\")\n",
    "dim_count={}\n",
    "\n",
    "for route_info in data:\n",
    "    for trip_info in route_info[\"route\"]:\n",
    "        \n",
    "        set_of_items=set()\n",
    "        \n",
    "        for merchandise in trip_info[\"merchandise\"]:\n",
    "            set_of_items.add(merchandise)\n",
    "\n",
    "        conn_name=trip_info[\"from\"]+\"-\"+trip_info[\"to\"]\n",
    "        \n",
    "        if conn_name not in dim_count:\n",
    "            dim_count[conn_name]=set_of_items\n",
    "        else:\n",
    "            dim_count[conn_name].update(set_of_items)\n",
    "\n",
    "#create a mapping method by converting the sets (first for speed) into\n",
    "#tuples which take less memory than lists\n",
    "#we do this so we can say that 'pens' is for example the first dimension\n",
    "#and 'milk' the second\n",
    "for conn_name in dim_count:\n",
    "    dim_count[conn_name]=(tuple(dim_count[conn_name]))\n",
    "print(\"found \"+str(len(dim_count))+\" subspaces:\")\n",
    "print(dim_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the data into lists of data points so the clustering can be applied\n",
    "print(\"converting the data into data points for each subspaces\")\n",
    "data_points={}\n",
    "for conn_name in dim_count:\n",
    "    data_points[conn_name]=[]\n",
    "\n",
    "for route_info in data:\n",
    "    for trip_info in route_info[\"route\"]:\n",
    "        conn_name=trip_info[\"from\"]+\"-\"+trip_info[\"to\"]\n",
    "        \n",
    "        temp_point=[0] * len(dim_count[conn_name])\n",
    "        \n",
    "        for merch in trip_info[\"merchandise\"]:\n",
    "            index=dim_count[conn_name].index(merch)\n",
    "            temp_point[index]=trip_info[\"merchandise\"][merch]\n",
    "                    \n",
    "        data_points[conn_name].append(temp_point) #change to list again if needed\n",
    "print(\"done\")\n",
    "print(data_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "#ignore warnings temporarily for better readability\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def sample(upper_limit1, amount_of_samples1, data1):\n",
    "    rand_numbs=random.sample(range(0, upper_limit1), amount_of_samples1)\n",
    "    sample_space=[list(data1[i]) for i in rand_numbs]\n",
    "    return sample_space\n",
    "\n",
    "def round_list(lst, decimal_places=1):\n",
    "    rounded_list = [round(element, decimal_places) for element in lst]\n",
    "    return rounded_list\n",
    "\n",
    "ext_data_points={}\n",
    "clusterinfo={}\n",
    "count=0\n",
    "for city in data_points:\n",
    "    count=count+1\n",
    "    max_expected_clusters=0\n",
    "    labeling={}\n",
    "    data = data_points[city]\n",
    "    #limit the sample space\n",
    "    datasize=len(data)\n",
    "    upper_limit=datasize\n",
    "    if datasize<1000:\n",
    "        amount_of_samples=min(datasize, 100)\n",
    "        max_expected_clusters=round(float(amount_of_samples)**(1/2))\n",
    "    elif datasize<10000:\n",
    "        amount_of_samples=round(datasize/10)\n",
    "        max_expected_clusters=round(float(amount_of_samples)**(1/2))\n",
    "\n",
    "    else:\n",
    "        amount_of_samples=round(datasize/100)\n",
    "        max_expected_clusters=round(float(amount_of_samples)**(1/2))\n",
    "\n",
    "    sample_space=[]\n",
    "    \n",
    "    #check if all the samplepoint    print(amount_of_samples)\n",
    "    #are the same and if they are try again 5 times\n",
    "    #if still the same pass the knowledge on\n",
    "    for _ in range(5):\n",
    "        one_point_marker=True\n",
    "        sample_space=sample(upper_limit, amount_of_samples, data)\n",
    "        if all(x == data[0] for x in sample_space) != True:\n",
    "            one_point_marker=False\n",
    "            break\n",
    "            \n",
    "    # Calculate silhouette scores for different values of k using samples \n",
    "    silhouette_scores = []\n",
    "    #limit expected amount of clusters proportional to the amount of datapoints we can maybe say that for every 2% of dataset there may be a cluster existing\n",
    "    \n",
    "    K_range = range(2, max_expected_clusters)\n",
    "\n",
    "    print(str(count)+\"/\"+str(len(data_points))+\" trying to find \"+str(max_expected_clusters)+ \" clusters for subspace: \"+ city +\" in \" + str(amount_of_samples)+\" samples with datasize of \" + str(datasize))\n",
    "\n",
    "    \n",
    "    if one_point_marker==False and max_expected_clusters>2:\n",
    "        #loop through possible amount of clusters and stop for first silhouette with score >0.7\n",
    "        for k in K_range:\n",
    "            try:\n",
    "                scaler = MinMaxScaler()\n",
    "                sample_space = scaler.fit_transform(sample_space)\n",
    "                kmeans = KMeans(n_clusters=k)\n",
    "                kmeans.fit(sample_space)\n",
    "                labels = kmeans.labels_\n",
    "                silhouette_scores.append(silhouette_score(sample_space, labels))\n",
    "            except ConvergenceWarning as e:\n",
    "                break\n",
    "            if max(silhouette_scores)>0.7:\n",
    "                break\n",
    "        \n",
    "        #determine k using the sampled space\n",
    "        max_ss=max(silhouette_scores)\n",
    "        if max_ss>0.6:\n",
    "            k=silhouette_scores.index(max_ss)+2\n",
    "        else:\n",
    "            k=1\n",
    "            \n",
    "    else:\n",
    "        k=1\n",
    "    print(\"found \" + str(k) + \" clusters. Now perform clustering on the data:\")\n",
    "    #now use kmeans to find the labels of the points for the entire set\n",
    "    scaler = MinMaxScaler()\n",
    "    data = scaler.fit_transform(data)\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(data)\n",
    "\n",
    "    print(\"done\")\n",
    "    print(\"building map with labels and cluster info\")\n",
    "    #build the extended data points set which also contain the labels of the points\n",
    "    labels=kmeans.labels_\n",
    "    \n",
    "    for index, point in enumerate(data_points[city]):\n",
    "        labeling[tuple(point)]=labels[index]\n",
    "    ext_data_points[city]=labeling\n",
    "    \n",
    "    #build a dataset which contains info about each cluster\n",
    "    for i in range(k):\n",
    "        temp={}\n",
    "        oneclusterdata=data[labels==i]\n",
    "        temp[\"count\"]=(len(oneclusterdata))\n",
    "        kmeans1=KMeans(n_clusters=1)\n",
    "        kmeans1.fit(oneclusterdata)\n",
    "        temp[\"inertia\"]=(kmeans1.inertia_)\n",
    "        temp[\"centroid\"]=tuple(round_list(scaler.inverse_transform(kmeans.cluster_centers_)[i]))\n",
    "        clusterinfo[city+\"-\"+str(i)]=temp\n",
    "print(\"found \"+str(len(clusterinfo))+\" clusters\")\n",
    "\n",
    "print(ext_data_points)\n",
    "print(clusterinfo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now build the new, transformed dataset out of the old dataset and the gathered data\n",
    "#the data we gathered are the mapping tool (dim_count) and the ext_data_points\n",
    "with open(filename, 'r') as file:\n",
    "    data = json.load(file)\n",
    "data=data[0:10000]\n",
    "\n",
    "dataset=[]\n",
    "dim_map=dim_count\n",
    "clust_map=ext_data_points\n",
    "\n",
    "for route_info in data:\n",
    "    conv_route=[]\n",
    "    cont_trips=[]\n",
    "    \n",
    "    \n",
    "    for trip_info in route_info[\"route\"]:\n",
    "        conn_name=trip_info[\"from\"]+\"-\"+trip_info[\"to\"]\n",
    "        \n",
    "        temp_point=[0] * len(dim_map[conn_name])\n",
    "        \n",
    "        for merch in trip_info[\"merchandise\"]:\n",
    "            index=dim_map[conn_name].index(merch)\n",
    "            temp_point[index]=trip_info[\"merchandise\"][merch]\n",
    "        cluster=clust_map[conn_name][tuple(temp_point)]\n",
    "        trip_name=conn_name+\"-\"+str(cluster)\n",
    "        cont_trips.append(trip_name)\n",
    "        \n",
    "    conv_route.append(route_info[\"id\"])\n",
    "    conv_route.append(route_info[\"driver\"])\n",
    "    conv_route.append(route_info[\"sroute\"])\n",
    "    conv_route.append(tuple(cont_trips))\n",
    "    dataset.append(tuple(conv_route))\n",
    "    \n",
    "dataset=tuple(dataset)\n",
    "for x in dataset:\n",
    "    print(x)\n",
    "\n",
    "\n",
    "#the structure is as follows:\n",
    "#(id, driver, sroute, (city1-city2-clusternumber))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the output variables are:\n",
    "print(clusterinfo) #contains all info about the clusters\n",
    "print(dim_map) #needed to find the mapping of the clusters (which dimension is which merchandise)\n",
    "print(dataset) #converted actual routes using the clusters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
