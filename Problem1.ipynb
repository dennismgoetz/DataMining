{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename=\"actual_routes.json\"\n",
    "prints=True\n",
    "\n",
    "\n",
    "#import data\n",
    "import json\n",
    "if(prints==True):print(\"loading data into memory\")\n",
    "with open(filename, 'r') as file:\n",
    "    data = json.load(file)\n",
    "if(prints==True):print(\"loaded \"+str(len(data))+\" routes into memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the amount of dimensions (possible merch) for all the possible \n",
    "#connections between cities \n",
    "if(prints==True):print(\"determine subspaces and its dimensions\")\n",
    "dim_count={}\n",
    "\n",
    "for route_info in data:\n",
    "    for trip_info in route_info[\"route\"]:\n",
    "        \n",
    "        set_of_items=set()\n",
    "        \n",
    "        for merchandise in trip_info[\"merchandise\"]:\n",
    "            set_of_items.add(merchandise)\n",
    "\n",
    "        conn_name=trip_info[\"from\"]+\"-\"+trip_info[\"to\"]\n",
    "        \n",
    "        if conn_name not in dim_count:\n",
    "            dim_count[conn_name]=set_of_items\n",
    "        else:\n",
    "            dim_count[conn_name].update(set_of_items)\n",
    "\n",
    "#create a mapping method by converting the sets (first for speed) into\n",
    "#tuples which take less memory than lists\n",
    "#we do this so we can say that 'pens' is for example the first dimension\n",
    "#and 'milk' the second\n",
    "for conn_name in dim_count:\n",
    "    dim_count[conn_name]=(tuple(dim_count[conn_name]))\n",
    "if(prints==True):print(\"found \"+str(len(dim_count))+\" subspaces:\")\n",
    "#if(prints==True):print(dim_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the data into lists of data points so the clustering can be applied\n",
    "if(prints==True):print(\"converting the data into data points for each subspaces\")\n",
    "data_points={}\n",
    "for conn_name in dim_count:\n",
    "    data_points[conn_name]=[]\n",
    "\n",
    "for route_info in data:\n",
    "    for trip_info in route_info[\"route\"]:\n",
    "        conn_name=trip_info[\"from\"]+\"-\"+trip_info[\"to\"]\n",
    "        \n",
    "        temp_point=[0] * len(dim_count[conn_name])\n",
    "        \n",
    "        for merch in trip_info[\"merchandise\"]:\n",
    "            index=dim_count[conn_name].index(merch)\n",
    "            temp_point[index]=trip_info[\"merchandise\"][merch]\n",
    "                    \n",
    "        data_points[conn_name].append(temp_point) #change to list again if needed\n",
    "if(prints==True):print(\"done\")\n",
    "#if(prints==True):print(data_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages/joblib/externals/loky/backend/context.py\", line 217, in _count_physical_cores\n",
      "    raise ValueError(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ConvergenceWarning' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 72\u001b[0m\n\u001b[1;32m     71\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39mk)\n\u001b[0;32m---> 72\u001b[0m kmeans\u001b[38;5;241m.\u001b[39mfit(sample_space)\n\u001b[1;32m     73\u001b[0m labels \u001b[38;5;241m=\u001b[39m kmeans\u001b[38;5;241m.\u001b[39mlabels_\n",
      "File \u001b[0;32m/opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1526\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;66;03m# run a k-means once\u001b[39;00m\n\u001b[0;32m-> 1526\u001b[0m labels, inertia, centers, n_iter_ \u001b[38;5;241m=\u001b[39m kmeans_single(\n\u001b[1;32m   1527\u001b[0m     X,\n\u001b[1;32m   1528\u001b[0m     sample_weight,\n\u001b[1;32m   1529\u001b[0m     centers_init,\n\u001b[1;32m   1530\u001b[0m     max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter,\n\u001b[1;32m   1531\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m   1532\u001b[0m     tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tol,\n\u001b[1;32m   1533\u001b[0m     n_threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_threads,\n\u001b[1;32m   1534\u001b[0m )\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# determine if these results are the best so far\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# we chose a new run if it has a better inertia and the clustering is\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;66;03m# different from the best so far (it's possible that the inertia is\u001b[39;00m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;66;03m# slightly better even if the clustering is the same with potentially\u001b[39;00m\n\u001b[1;32m   1540\u001b[0m \u001b[38;5;66;03m# permuted labels, due to rounding errors)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:740\u001b[0m, in \u001b[0;36m_kmeans_single_lloyd\u001b[0;34m(X, sample_weight, centers_init, max_iter, verbose, tol, n_threads)\u001b[0m\n\u001b[1;32m    728\u001b[0m         lloyd_iter(\n\u001b[1;32m    729\u001b[0m             X,\n\u001b[1;32m    730\u001b[0m             sample_weight,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    737\u001b[0m             update_centers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    738\u001b[0m         )\n\u001b[0;32m--> 740\u001b[0m inertia \u001b[38;5;241m=\u001b[39m _inertia(X, sample_weight, centers, labels, n_threads)\n\u001b[1;32m    742\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m labels, inertia, centers, i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 75\u001b[0m\n\u001b[1;32m     73\u001b[0m     labels \u001b[38;5;241m=\u001b[39m kmeans\u001b[38;5;241m.\u001b[39mlabels_\n\u001b[1;32m     74\u001b[0m     silhouette_scores\u001b[38;5;241m.\u001b[39mappend(silhouette_score(sample_space, labels))\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConvergenceWarning \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(silhouette_scores)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0.7\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ConvergenceWarning' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "#ignore warnings temporarily for better readability\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def sample(upper_limit1, amount_of_samples1, data1):\n",
    "    rand_numbs=random.sample(range(0, upper_limit1), amount_of_samples1)\n",
    "    sample_space=[list(data1[i]) for i in rand_numbs]\n",
    "    return sample_space\n",
    "\n",
    "def round_list(lst, decimal_places=1):\n",
    "    rounded_list = [round(element, decimal_places) for element in lst]\n",
    "    return rounded_list\n",
    "\n",
    "ext_data_points={}\n",
    "clusterinfo={}\n",
    "count=0\n",
    "for city in data_points:\n",
    "    count=count+1\n",
    "    max_expected_clusters=0\n",
    "    labeling={}\n",
    "    data = data_points[city]\n",
    "    #limit the sample space\n",
    "    datasize=len(data)\n",
    "    upper_limit=datasize\n",
    "    if datasize<1000:\n",
    "        amount_of_samples=min(datasize, 100)\n",
    "        max_expected_clusters=round(float(amount_of_samples)**(1/2))\n",
    "    elif datasize<10000:\n",
    "        amount_of_samples=round(datasize/10)\n",
    "        max_expected_clusters=round(float(amount_of_samples)**(1/2))\n",
    "\n",
    "    else:\n",
    "        amount_of_samples=round(datasize/100)\n",
    "        max_expected_clusters=round(float(amount_of_samples)**(1/2))\n",
    "\n",
    "    sample_space=[]\n",
    "    \n",
    "    #check if all the samplepoint    print(amount_of_samples)\n",
    "    #are the same and if they are try again 5 times\n",
    "    #if still the same pass the knowledge on\n",
    "    for _ in range(5):\n",
    "        one_point_marker=True\n",
    "        sample_space=sample(upper_limit, amount_of_samples, data)\n",
    "        if all(x == data[0] for x in sample_space) != True:\n",
    "            one_point_marker=False\n",
    "            break\n",
    "            \n",
    "    # Calculate silhouette scores for different values of k using samples \n",
    "    silhouette_scores = []\n",
    "    #limit expected amount of clusters proportional to the amount of datapoints we can maybe say that for every 2% of dataset there may be a cluster existing\n",
    "    \n",
    "    K_range = range(2, max_expected_clusters)\n",
    "\n",
    "    if(prints==True):print(str(count)+\"/\"+str(len(data_points))+\" trying to find \"+str(max_expected_clusters)+ \" clusters for subspace: \"+ city +\" in \" + str(amount_of_samples)+\" samples with datasize of \" + str(datasize))\n",
    "\n",
    "    \n",
    "    if one_point_marker==False and max_expected_clusters>2:\n",
    "        #loop through possible amount of clusters and stop for first silhouette with score >0.7\n",
    "        for k in K_range:\n",
    "            try:\n",
    "                scaler = MinMaxScaler()\n",
    "                sample_space = scaler.fit_transform(sample_space)\n",
    "                kmeans = KMeans(n_clusters=k)\n",
    "                kmeans.fit(sample_space)\n",
    "                labels = kmeans.labels_\n",
    "                silhouette_scores.append(silhouette_score(sample_space, labels))\n",
    "            except ConvergenceWarning as e:\n",
    "                break\n",
    "            if max(silhouette_scores)>0.7:\n",
    "                break\n",
    "        \n",
    "        #determine k using the sampled space\n",
    "        max_ss=max(silhouette_scores)\n",
    "        if max_ss>0.6:\n",
    "            k=silhouette_scores.index(max_ss)+2\n",
    "        else:\n",
    "            k=1\n",
    "            \n",
    "    else:\n",
    "        k=1\n",
    "    if(prints==True):print(\"found \" + str(k) + \" clusters. Now perform clustering on the data:\")\n",
    "    #now use kmeans to find the labels of the points for the entire set\n",
    "    scaler = MinMaxScaler()\n",
    "    data = scaler.fit_transform(data)\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(data)\n",
    "\n",
    "    if(prints==True):print(\"done\")\n",
    "    if(prints==True):print(\"building map with labels and cluster info\")\n",
    "    #build the extended data points set which also contain the labels of the points\n",
    "    labels=kmeans.labels_\n",
    "    \n",
    "    for index, point in enumerate(data_points[city]):\n",
    "        labeling[tuple(point)]=labels[index]\n",
    "    ext_data_points[city]=labeling\n",
    "    \n",
    "    #build a dataset which contains info about each cluster\n",
    "    for i in range(k):\n",
    "        temp={}\n",
    "        oneclusterdata=data[labels==i]\n",
    "        temp[\"count\"]=(len(oneclusterdata))\n",
    "        kmeans1=KMeans(n_clusters=1)\n",
    "        kmeans1.fit(oneclusterdata)\n",
    "        temp[\"inertia\"]=(kmeans1.inertia_)\n",
    "        temp[\"centroid\"]=tuple(round_list(scaler.inverse_transform(kmeans.cluster_centers_)[i]))\n",
    "        clusterinfo[city+\"-\"+str(i)]=temp\n",
    "if(prints==True):print(\"found \"+str(len(clusterinfo))+\" clusters\")\n",
    "\n",
    "#if(prints==True):print(ext_data_points)\n",
    "#if(prints==True):print(clusterinfo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now build the new, transformed dataset out of the old dataset and the gathered data\n",
    "#the data we gathered are the mapping tool (dim_count) and the ext_data_points\n",
    "with open(filename, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "dataset=[]\n",
    "dim_map=dim_count\n",
    "clust_map=ext_data_points\n",
    "\n",
    "for route_info in data:\n",
    "    conv_route=[]\n",
    "    cont_trips=[]\n",
    "    \n",
    "    \n",
    "    for trip_info in route_info[\"route\"]:\n",
    "        conn_name=trip_info[\"from\"]+\"-\"+trip_info[\"to\"]\n",
    "        \n",
    "        temp_point=[0] * len(dim_map[conn_name])\n",
    "        \n",
    "        for merch in trip_info[\"merchandise\"]:\n",
    "            index=dim_map[conn_name].index(merch)\n",
    "            temp_point[index]=trip_info[\"merchandise\"][merch]\n",
    "        cluster=clust_map[conn_name][tuple(temp_point)]\n",
    "        trip_name=conn_name+\"-\"+str(cluster)\n",
    "        cont_trips.append(trip_name)\n",
    "        \n",
    "    conv_route.append(route_info[\"id\"])\n",
    "    conv_route.append(route_info[\"driver\"])\n",
    "    conv_route.append(route_info[\"sroute\"])\n",
    "    conv_route.append(tuple(cont_trips))\n",
    "    dataset.append(tuple(conv_route))\n",
    "    \n",
    "dataset=tuple(dataset)\n",
    "#for x in dataset:\n",
    "#    if(prints==True):print(x)\n",
    "\n",
    "\n",
    "#the structure is as follows:\n",
    "#(id, driver, sroute, (city1-city2-clusternumber))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the singletons not frequent enough, depending on a certain thresehold. \n",
    "#In this case, I've chosen 100. \n",
    "#The function outputs a tuple with the frequent items and not frequently enough items.\n",
    "def prune(X,thresehold):\n",
    "    reduce=[]\n",
    "    for i in X:\n",
    "        if X[i]<thresehold:\n",
    "            reduce.append(((i,),X[i]))\n",
    "    for i in reduce:\n",
    "        X.pop(i[0][0])\n",
    "    return (X,reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function that, given two trips, outputs wether they could be chained or not,\n",
    "#for example, if the first trip ends in Milano, it tells you wether the second one\n",
    "#starts in Milano too. In this way, we only create \"possible couples\" with the next function.\n",
    "def match(x,y):\n",
    "    w1=x\n",
    "    l1=len(w1)\n",
    "    w1=w1[:l1-2]\n",
    "    index1=w1.index('-')\n",
    "    w1=w1[index1+1:]\n",
    "    \n",
    "    w2=y\n",
    "    l2=len(w2)\n",
    "    w2=w2[:l2-2]\n",
    "    index2=w2.index('-')\n",
    "    w2=w2[:index2]\n",
    "    \n",
    "    return w1==w2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the above function we create couples.It also gives us a list \n",
    "#of the singletons that we could not extend, and a list of the ones we could.\n",
    "#We need the latter in case the couples don't pass the prune in the next step;\n",
    "#in that case, we would go back one step and offer the routes that created that couple.\n",
    "\n",
    "def couples(X):\n",
    "    nextcandidates=[]\n",
    "    extend=[]\n",
    "    for i in X:\n",
    "        extend.append(((i,),X[i]))\n",
    "    lista=[]\n",
    "    for i in X:\n",
    "        for j in X:\n",
    "            if match(i,j):\n",
    "                if ((i,),X[i]) in extend:\n",
    "                    extend.remove(((i,),X[i]))\n",
    "                if ((j,),X[j]) in extend:\n",
    "                    extend.remove(((j,),X[j]))\n",
    "                    \n",
    "                if (i,j) not in lista:\n",
    "                    lista.append((i,j))\n",
    "                if ((i,),X[i]) not in nextcandidates:\n",
    "                        nextcandidates.append(((i,),X[i]))\n",
    "                if ((j,),X[j]) not in nextcandidates:\n",
    "                        nextcandidates.append(((j,),X[j]))\n",
    "    return (lista,extend,nextcandidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function tells us wether a given sequence is a subsequence of a longer one.\n",
    "#basically we are certifying if a given subroute is part of a bigger route in the dataset.\n",
    "\n",
    "def subsequence(m,S):\n",
    "    l=len(m)\n",
    "    L=len(S)\n",
    "    for i in range(0,L-l+1):\n",
    "        if S[i:i+l]==m:\n",
    "            return True\n",
    "    return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#given possible frequent tuples it creates a dictionary with the occurrences\n",
    "#of such tuples as subsequences in the dataset.\n",
    "\n",
    "def find(candidates,X):\n",
    "    freq={}\n",
    "    for i in candidates:\n",
    "        for j in X:\n",
    "            if subsequence(i,j[3]):\n",
    "                if i in freq:\n",
    "                    freq[i]=freq[i]+1\n",
    "                else:\n",
    "                    freq[i]=1\n",
    "    return freq\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function takes a list of tuples of the same length and combines them\n",
    "#to create new tuples of length + 1. Furthermore, it only creates \"logical\"\n",
    "#tuples. That is, it won't combine (1,2,3) with (1,2,5) but it will combine \n",
    "#(1,2,3) with (2,3,4) to create (1,2,3,4). It also gives as a list of tuples\n",
    "#we could not extend, and the ones we could, for analogous reasons as in the \n",
    "#case of the function couples.\n",
    "\n",
    "def combine(X):\n",
    "    extend=[]\n",
    "    for i in X:\n",
    "        extend.append((i,X[i]))\n",
    "    nextcandidates=[]\n",
    "    lista=[]\n",
    "    if len(X)!=0:\n",
    "        for i in X:\n",
    "            l=len(i)\n",
    "            break\n",
    "        for tuple1 in X:\n",
    "            for tuple2 in X:\n",
    "                if tuple2[:l-1] == tuple1[1:]:\n",
    "                    newtuple=tuple1 + (tuple2[l-1],)\n",
    "                    if (tuple1,X[tuple1]) in extend:\n",
    "                        extend.remove((tuple1,X[tuple1]))\n",
    "                    if (tuple2,X[tuple2]) in extend:\n",
    "                        extend.remove((tuple2,X[tuple2]))\n",
    "                    if newtuple not in lista:\n",
    "                        lista.append(newtuple)\n",
    "                    if (tuple1,X[tuple1]) not in nextcandidates:\n",
    "                        nextcandidates.append((tuple1,X[tuple1]))\n",
    "                    if (tuple2,X[tuple2]) not in nextcandidates:\n",
    "                        nextcandidates.append((tuple2,X[tuple2]))\n",
    "    return (lista,extend,nextcandidates)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It takes a trip-clusters and converts it into an actual trip.\n",
    "\n",
    "def cluster_to_info(x,clusterinfo,dim_map):\n",
    "    \n",
    "    w1=x\n",
    "    l1=len(w1)\n",
    "    w1=w1[:l1-2]\n",
    "    index1=w1.index('-')\n",
    "    w1=w1[index1+1:]\n",
    "    \n",
    "    w2=x[:l1-2]\n",
    "    index2=w2.index('-')\n",
    "    w2=w2[:index2]\n",
    "    \n",
    "    trip={}\n",
    "    trip['from']=w2\n",
    "    \n",
    "    \n",
    "    info=clusterinfo[x]['centroid']\n",
    "    l=len(x)-2\n",
    "    merch=dim_map[x[:l]]\n",
    "    s={}\n",
    "    for i in range(len(merch)):\n",
    "         if info[i]!=0:\n",
    "            s[merch[i]] = info[i]\n",
    "            \n",
    "    trip['merchandise']=s\n",
    "    trip['to']=w1\n",
    "    \n",
    "    return trip\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It takes all of the cluster-routes and convert them into actual routes.\n",
    "\n",
    "\n",
    "def offeroutes(OFFER):\n",
    "    \n",
    "    lista=[]\n",
    "    H=len(OFFER)\n",
    "    for i in range(H):\n",
    "        ruta={}\n",
    "        ruta['id']='s'+str(i+1)\n",
    "        TRIPS=[]\n",
    "        for j in OFFER[i][0]:\n",
    "            trip= cluster_to_info(j,clusterinfo,dim_map)\n",
    "            TRIPS.append(trip)\n",
    "        ruta['route']=TRIPS\n",
    "        lista.append((ruta,OFFER[i][1]))\n",
    "    L=[]\n",
    "    for i in lista:\n",
    "        L.append(i[1])\n",
    "    L.sort()\n",
    "    L=L[::-1]\n",
    "    Lista=[]\n",
    "    for i in L:\n",
    "        for j in lista:\n",
    "            if i==j[1]:\n",
    "                Lista.append(j[0)\n",
    "                lista.remove(j)\n",
    "                break   \n",
    "    for i in range(H):\n",
    "        Lista[i][0]['id']='s'+str(i+1)\n",
    "    return Lista\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FREQUENT_ITEMS(dataset,thresehold,NS):\n",
    "    OFFER=[]\n",
    "    \n",
    "        \n",
    "    freq={}\n",
    "    for x in dataset:\n",
    "        for i in x[3]:\n",
    "            if i not in freq:\n",
    "                freq[i] = 1\n",
    "            else:\n",
    "                freq[i] = freq[i] +1\n",
    "    \n",
    "    #we can also use clusterinfo to get freq\n",
    "    #freq={}\n",
    "    #for i in clusterinfo:\n",
    "    #    freq[i]=clusterinfo[i]['count']\n",
    "    \n",
    "    \n",
    "    S = prune(freq,thresehold)[0]\n",
    "    (candidates,extend,nextcandidates)=couples(S)\n",
    "    OFFER=OFFER + extend\n",
    "    \n",
    "    \n",
    "    while len(candidates)!=0:\n",
    "        new=find(candidates,dataset)\n",
    "        (S,H) = prune(new,thresehold)\n",
    "        count=0\n",
    "        for j in nextcandidates:\n",
    "            for i in H:\n",
    "                if subsequence(j[0],i[0]):\n",
    "                    OFFER=OFFER + [j]\n",
    "                    count=count+1\n",
    "            if count==0:\n",
    "                for i in S:\n",
    "                    if subsequence(j[0],i):\n",
    "                        count=count+1\n",
    "            if count==0:\n",
    "                OFFER=OFFER + [j]\n",
    "                \n",
    "        (candidates,extend,nextcandidates)=combine(S)\n",
    "        OFFER = OFFER+ extend\n",
    "        \n",
    "    return offeroutes(OFFER)\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OFFER_ROUTES(dataset,NS):\n",
    "    n=0\n",
    "    OFFER= FREQUENT_ITEMS(dataset,len(dataset)/(2**n))\n",
    "    while len(OFFER)<NS:\n",
    "        n=n+1\n",
    "        OFFER=FREQUENT_ITEMS(dataset,len(dataset)/(2**n))\n",
    "    return (OFFER[:NS])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
